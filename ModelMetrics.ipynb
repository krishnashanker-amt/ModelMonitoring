{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Monitoring Automation\n",
    "\n",
    "This notebook documents the code and progress as I work my way towards setting up an automated process to monitor the performance of the fraud model(s), across partners and products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trellis\n",
    "import os\n",
    "from avant_python_utils.email import send_email\n",
    "from datalaketools.connectors.presto_db import PrestoDB\n",
    "presto = PrestoDB()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, f1_score,recall_score,precision_score, average_precision_score\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants with Column Names\n",
    "\n",
    "Lots of columns that are used in multiple functions throughout this document. I'll define those columns here so I won't have to change them in multiple places later if there's a change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_COL = 'score_5'\n",
    "YPRED_COL = 'prediction'\n",
    "YTRUE_COL = 'suspected_fraud'\n",
    "TIME_COL = 'loan_processing_start_time'\n",
    "THRESHOLD = 0.05\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trellis.start()\n",
    "# fraud = trellis.connect('us_fraud_follower')\n",
    "#parent_dir_path = os.path.dirname(os.path.abspath(__file__)) - REMOVE COMMENT IN PYTHON SCRIPT\n",
    "parent_dir_path = os.getcwd()\n",
    "subject = 'Avant Model Monitor Weekly Report (Data Only)'\n",
    "credentials = {'username': trellis.keys('automate_email')['email'], 'password': trellis.keys('automate_email')['pw']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL query parameters\n",
    "loan_window = 'week'\n",
    "start_date = '2018-09-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to obtain data at a loan ID level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = presto.execute_df('''\n",
    "SELECT\n",
    "  l.id as loan_id\n",
    ", l.loan_processing_start_time\n",
    ", date_trunc('{LOAN_WINDOW}', l.loan_processing_start_time) as entered_lp_week\n",
    ", l.status\n",
    ", case when l.status in ('current','late','paid_off','charged_off') then 1 else 0 end as issued\n",
    ", case when c.high_confidence_fraud_indicator=true or cfl.id is not null then 1 else 0 end as high_confidence_fraud_indicator\n",
    ", case when cfr.customer_id is not null then 1 else 0 end as suspected_fraud \n",
    "--, cfrt.name as fraud_reason\n",
    ", cast(fd.score_4 as double) as score_4\n",
    ", cast(fd.score_5 as double) as score_5\n",
    ", coalesce(cast(fd.score_5 as double), cast(fd.score_4 as double)) as hard_score\n",
    ", l.state\n",
    ", l.payment_method\n",
    ", l.loan_amount\n",
    ", ca.product_type\n",
    ", vrdt.risk_summary_identity_high\n",
    ", vrdt.risk_summary_identity_medium\n",
    ", vrdt.risk_summary_identity_low\n",
    "FROM avant.dw.customer_applications ca\n",
    "LEFT JOIN avant.dw.loans l on l.customer_application_id = ca.id\n",
    "JOIN avant.dw.customers c\n",
    "  ON c.id = l.customer_id\n",
    "  \n",
    "  -- getting dependent variable\n",
    "  \n",
    "LEFT JOIN (\n",
    "select customer_id \n",
    "from avant.avant_basic.customer_fraud_reasons cfr \n",
    "group by 1\n",
    ") cfr on c.id = cfr.customer_id\n",
    "  \n",
    " -- LEFT JOIN avant.avant_basic.customer_fraud_reason_types cfrt on cfr.customer_fraud_reason_type_id = cfrt.id\n",
    "  \n",
    "  -- getting fraud scores\n",
    "LEFT JOIN (\n",
    "  SELECT\n",
    "    l.id as loan_id\n",
    "  , json_extract_scalar(fd.model_scores, '$[\"fraud/en-US/4.1.0\"][\"score\"]') as score_4\n",
    "  , json_extract_scalar(fd.model_scores, '$[\"fraud/en-US/5.0.0\"][\"score\"]') as score_5\n",
    "  , fd.id as fraud_decision_id\n",
    "  , row_number() over (partition by l.id order by fd.created_at desc) as row_num\n",
    "  FROM avant.dw.loans l\n",
    "  JOIN avant.avant_basic.fraud_decisions fd\n",
    "    ON fd.customer_application_id = l.customer_application_id\n",
    "    AND fd.created_at AT TIME ZONE 'America/Chicago' >= l.loan_processing_start_time\n",
    "WHERE l.loan_processing_start_time > date '{START_DATE}'\n",
    ") fd \n",
    "  ON fd.loan_id = l.id \n",
    "  AND fd.row_num=1\n",
    "  -- getting fraud indicator\n",
    "LEFT JOIN avant.avant_basic.confirmed_fraud_logs cfl \n",
    "  ON cfl.customer_id = c.id\n",
    "  \n",
    "    -- filtering for valid loans to evaluate performance on\n",
    "  -- JOIN avant.dw.loan_performance_by_installment lp \n",
    "  -- ON lp.loan_id = l.id \n",
    "  -- AND lp.installment_number = 1\n",
    "  -- AND lp.installment_date <= date_add('day', -64, current_timestamp)\n",
    "\n",
    "  \n",
    "  -- adding identity tier a loan was assigned to and fraud_review flag\n",
    "  LEFT JOIN avant.dw_temp_newver.verifications_risks_decisions_test vrdt\n",
    "  on ca.id = vrdt.customer_application_id and vrdt.row_num_recent = 1\n",
    "  \n",
    "  \n",
    "WHERE l.loan_processing_start_time > date '{START_DATE}'\n",
    "'''.format(LOAN_WINDOW = loan_window, START_DATE = start_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove records with no fraud score\n",
    "df = df_raw[df_raw.score_5.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda-envs/gkrishna_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[YPRED_COL] = np.where(df[SCORE_COL] > THRESHOLD, 1, 0)\n",
    "#df['prediction'] = [1 if x > 0.05 else 0 for x in df['score_5']]\n",
    "#df['prediction'] = list(np.where(df['score_5'] > 0.05, 1, 0)) \n",
    "# All three solutions raise the settingwithcopywarining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_COL = 'score_5'\n",
    "YPRED_COL = 'prediction'\n",
    "YTRUE_COL = 'suspected_fraud'\n",
    "TIME_COL = 'loan_processing_start_time'\n",
    "THRESHOLD = 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weeklyEvaluator(dframe, ytrue = YTRUE_COL, ypred = YPRED_COL, scores = SCORE_COL):\n",
    "    true_positives = dframe[ytrue] * dframe[ypred]\n",
    "    false_positives = (1-dframe[ytrue]) * dframe[ypred]\n",
    "    false_negatives =  dframe[ytrue] * (1-dframe[ypred])\n",
    "    \n",
    "    #calculating multiple metrics\n",
    "    precision = precision_score(y_true = dframe[ytrue], y_pred = dframe[ypred], pos_label = 1)\n",
    "    recall = recall_score(y_true = dframe[ytrue], y_pred = dframe[ypred], pos_label = 1)\n",
    "    f1score = f1_score(y_true = dframe[ytrue], y_pred = dframe[ypred], pos_label = 1)\n",
    "    auc_pr = average_precision_score(y_true = dframe[ytrue], y_score = dframe[scores], pos_label=1)\n",
    "    auc_roc = roc_auc_score(y_true = dframe[ytrue], y_score = dframe[scores])\n",
    "    fraud_rate = dframe[ytrue].sum()/len(dframe.index)\n",
    "    avg_score = dframe[scores].sum()/len(dframe.index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pd.Series({'precision': precision, 'recall': recall, 'f1score': f1score, 'auc_pr':auc_pr, 'auc_roc':auc_roc,\n",
    "                     'fraud_rate': fraud_rate, 'avg_score': avg_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "byWeek_stats = df.groupby('entered_lp_week', as_index = False).apply(weeklyEvaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The scope is always look like this so we did not need to change anything\n",
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "#Name of our Service Account Key\n",
    "google_key_file = 'service_key.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '14ROlpuOP9IkixM5-nn1Pc0ux6kWgmj7c62NzdDl-5hU',\n",
       " 'updatedRange': \"'Raw Data'!A1:H104\",\n",
       " 'updatedRows': 104,\n",
       " 'updatedColumns': 8,\n",
       " 'updatedCells': 832}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the Worksheet ID\n",
    "wb = gc.open_by_key('14ROlpuOP9IkixM5-nn1Pc0ux6kWgmj7c62NzdDl-5hU')\n",
    "#This is the sheet name\n",
    "wks_name = 'Raw Data'\n",
    "ws = wb.worksheet(wks_name)\n",
    "\n",
    "#clear the existing data in worksheet\n",
    "ws.clear()\n",
    "\n",
    "#update new data to worksheet (first list out columns, and then add values for each column)\n",
    "ws.update([byWeek_stats.columns.values.tolist()] + byWeek_stats.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '14ROlpuOP9IkixM5-nn1Pc0ux6kWgmj7c62NzdDl-5hU',\n",
       " 'replies': [{}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.format(\"A2:A1000\", { \"numberFormat\": { \"type\": ('DATE') }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Table Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each metric, I need the following \n",
    "\n",
    "- Value since the date model was trained\n",
    "- Compare last 30 days to previous 30 days (with scope for excluding the last 2 months)\n",
    "- Absolute difference between value when model was trained, and current value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14453518028874277"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true = df['suspected_fraud'], y_pred = df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (date.today() - timedelta(days = 30)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-42-a0c9378eb023>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-a0c9378eb023>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a = 2018-09-15\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "a = '2018-09-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 10, 15, 0, 0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strptime(a, \"%Y-%m-%d\") + timedelta(days = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (date.today() - timedelta(days = 60)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.query('loan_processing_start_time > @b & loan_processing_start_time < @a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return a metric for a specified time period\n",
    "\n",
    "def metric_for_interval(from_interval, to_interval, metric):\n",
    "    ytrue = \n",
    "    ypred = \n",
    "    \n",
    "        data_first30 = dframe.query('@timecol > @modeltrain_date_start & @timecol < @modeltrain_date_end')\n",
    "        data_last30 = dframe.query('@timecol > @prev30_date & @timecol < @today_date')\n",
    "        data_prev30 = dframe.query('@timecol > @prev60_date & @timecol < @prev30_date')   \n",
    "    if metric == precision:\n",
    "        current_precision = precision_score(y_true = data_last30[ytrue], y_pred = data_last30[ypred], pos_label = 1)\n",
    "        initial_precision = precision_score(y_true = data_first30[ytrue], y_pred = data_first30[ypred], pos_label = 1)\n",
    "        prev30_precision = precision_score(y_true = data_prev30[ytrue], y_pred = data_prev30[ypred], pos_label = 1)\n",
    "    \n",
    "    if metric == recall:\n",
    "        #recall values\n",
    "        current_recall = recall_score(y_true = data_last30[ytrue], y_pred = data_last30[ypred], pos_label = 1)\n",
    "        initial_recall = recall_score(y_true = data_first30[ytrue], y_pred = data_first30[ypred], pos_label = 1)\n",
    "        prev30_recall = recall_score(y_true = data_prev30[ytrue], y_pred = data_prev30[ypred], pos_label = 1)   \n",
    "\n",
    "        \n",
    "SCORE_COL = 'score_5'\n",
    "YPRED_COL = 'prediction'\n",
    "YTRUE_COL = 'suspected_fraud'\n",
    "TIME_COL = 'loan_processing_start_time'\n",
    "THRESHOLD = 0.05\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/08/2020'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cellFiller(dframe, ytrue = 'suspected_fraud', ypred = 'prediction', scores = 'score_5', timecol = 'loan_processing_start_time'):\n",
    "    #Setting up variables with different date values\n",
    "    today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "    prev30_date = (date.today() - timedelta(days = 30)).strftime(\"%Y-%m-%d\")\n",
    "    prev60_date = (date.today() - timedelta(days = 60)).strftime(\"%Y-%m-%d\")\n",
    "    modeltrain_date_start = datetime.strptime('2018-09-15', \"%Y-%m-%d\")\n",
    "    modeltrain_date_end = (modeltrain_date + timedelta(days = 30)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #creating different datasets for the different time periods\n",
    "    \n",
    "    #dataset 1 - 30 days after model was trained\n",
    "    data_first30 = dframe.query('@timecol > @modeltrain_date_start & @timecol < @modeltrain_date_end')\n",
    "    data_last30 = dframe.query('@timecol > @prev30_date & @timecol < @today_date')\n",
    "    data_prev30 = dframe.query('@timecol > @prev60_date & @timecol < @prev30_date')   \n",
    "    \n",
    "    #precision values\n",
    "    current_precision = precision_score(y_true = data_last30[ytrue], y_pred = data_last30[ypred], pos_label = 1)\n",
    "    initial_precision = precision_score(y_true = data_first30[ytrue], y_pred = data_first30[ypred], pos_label = 1)\n",
    "    prev30_precision = precision_score(y_true = data_prev30[ytrue], y_pred = data_prev30[ypred], pos_label = 1)\n",
    "    \n",
    "    #recall values\n",
    "    current_recall = recall_score(y_true = data_last30[ytrue], y_pred = data_last30[ypred], pos_label = 1)\n",
    "    initial_recall = recall_score(y_true = data_first30[ytrue], y_pred = data_first30[ypred], pos_label = 1)\n",
    "    prev30_recall = recall_score(y_true = data_prev30[ytrue], y_pred = data_prev30[ypred], pos_label = 1)   \n",
    "    \n",
    "    #f1score\n",
    "    \n",
    "    output = {\"metric\": ['precision', 'recall','f1score', 'auc_pr', 'auc_roc', 'fraud_rate', 'avg_score'],\n",
    "             \"current_val\":[current_precision, current_recall]}\n",
    "    \n",
    "    \n",
    "                           \n",
    "                           \n",
    "            precision = tp.sum()/(tp.sum() + fp.sum())\n",
    "    recall = tp.sum()/(tp.sum() + fn.sum())\n",
    "    f1score = f1_score(y_true = dframe[ytrue], y_pred = dframe[ypred], pos_label = 1)\n",
    "    auc_pr = average_precision_score(y_true = dframe[ytrue], y_score = dframe[scores], pos_label=1)\n",
    "    auc_roc = roc_auc_score(y_true = dframe[ytrue], y_score = dframe[scores])\n",
    "    fraud_rate = dframe[ytrue].sum()/len(dframe.index)\n",
    "    avg_score = dframe[scores].sum()/len(dframe.index)                   \n",
    "                           \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-09-01'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Style 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Style 1\n",
    "\n",
    "d = {\"metric\":['recall', 'precision',''], \"current_val\":[0.5, 0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': ['recall', 'precision'], 'current_val': [0.5, 0.3]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>current_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric  current_val\n",
       "0     recall          0.5\n",
       "1  precision          0.3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style 2\n",
    "\n",
    "d = {\"recall\":[0.5, 0.7, 1], \"precision\":[0.9,0.8,0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recall  precision\n",
       "0     0.5        0.9\n",
       "1     0.7        0.8\n",
       "2     1.0        0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkrishna_env",
   "language": "python",
   "name": "gkrishna_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
