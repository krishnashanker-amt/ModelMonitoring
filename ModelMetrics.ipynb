{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Monitoring Automation\n",
    "\n",
    "This notebook documents the code and progress as I work my way towards setting up an automated process to monitor the performance of the fraud model(s), across partners and products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trellis\n",
    "import os\n",
    "from avant_python_utils.email import send_email\n",
    "from datalaketools.connectors.presto_db import PrestoDB\n",
    "presto = PrestoDB()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, f1_score,recall_score,precision_score, average_precision_score\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trellis.start()\n",
    "# fraud = trellis.connect('us_fraud_follower')\n",
    "#parent_dir_path = os.path.dirname(os.path.abspath(__file__)) - REMOVE COMMENT IN PYTHON SCRIPT\n",
    "parent_dir_path = os.getcwd()\n",
    "subject = 'Avant Model Monitor Weekly Report (Data Only)'\n",
    "credentials = {'username': trellis.keys('automate_email')['email'], 'password': trellis.keys('automate_email')['pw']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL query parameters\n",
    "loan_window = 'week'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to obtain data at a loan ID level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = presto.execute_df('''\n",
    "SELECT\n",
    "  l.id as loan_id\n",
    ", l.loan_processing_start_time\n",
    ", date_trunc('{LOAN_WINDOW}', l.loan_processing_start_time) as entered_lp_week\n",
    ", l.status\n",
    ", case when l.status in ('current','late','paid_off','charged_off') then 1 else 0 end as issued\n",
    ", case when c.high_confidence_fraud_indicator=true or cfl.id is not null then 1 else 0 end as high_confidence_fraud_indicator\n",
    ", cast(fd.score_4 as double) as score_4\n",
    ", cast(fd.score_5 as double) as score_5\n",
    ", coalesce(cast(fd.score_5 as double), cast(fd.score_4 as double)) as hard_score\n",
    ", l.state\n",
    ", l.payment_method\n",
    ", l.loan_amount\n",
    ", ca.product_type\n",
    ", vrdt.risk_summary_identity_high\n",
    ", vrdt.risk_summary_identity_medium\n",
    ", vrdt.risk_summary_identity_low\n",
    "FROM avant.dw.customer_applications ca\n",
    "LEFT JOIN avant.dw.loans l on l.customer_application_id = ca.id\n",
    "JOIN avant.dw.customers c\n",
    "  ON c.id = l.customer_id\n",
    "  \n",
    "  -- getting fraud scores\n",
    "LEFT JOIN (\n",
    "  SELECT\n",
    "    l.id as loan_id\n",
    "  , json_extract_scalar(fd.model_scores, '$[\"fraud/en-US/4.1.0\"][\"score\"]') as score_4\n",
    "  , json_extract_scalar(fd.model_scores, '$[\"fraud/en-US/5.0.0\"][\"score\"]') as score_5\n",
    "  , fd.id as fraud_decision_id\n",
    "  , row_number() over (partition by l.id order by fd.created_at desc) as row_num\n",
    "  FROM avant.dw.loans l\n",
    "  JOIN avant.avant_basic.fraud_decisions fd\n",
    "    ON fd.customer_application_id = l.customer_application_id\n",
    "    AND fd.created_at AT TIME ZONE 'America/Chicago' >= l.loan_processing_start_time\n",
    "WHERE l.loan_processing_start_time > date '2019-06-30'\n",
    ") fd \n",
    "  ON fd.loan_id = l.id \n",
    "  AND fd.row_num=1\n",
    "  -- getting fraud indicator\n",
    "LEFT JOIN avant.avant_basic.confirmed_fraud_logs cfl \n",
    "  ON cfl.customer_id = c.id\n",
    "  \n",
    "    -- filtering for valid loans to evaluate performance on\n",
    "  -- JOIN avant.dw.loan_performance_by_installment lp \n",
    "  -- ON lp.loan_id = l.id \n",
    "  -- AND lp.installment_number = 1\n",
    "  -- AND lp.installment_date <= date_add('day', -64, current_timestamp)\n",
    "\n",
    "  \n",
    "  -- adding identity tier a loan was assigned to and fraud_review flag\n",
    "  LEFT JOIN avant.dw_temp_newver.verifications_risks_decisions_test vrdt\n",
    "  on ca.id = vrdt.customer_application_id and vrdt.row_num_recent = 1\n",
    "WHERE l.loan_processing_start_time > date '2019-06-30'\n",
    "'''.format(LOAN_WINDOW = loan_window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove records with no fraud score\n",
    "df = df_raw[df_raw.score_5.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda-envs/gkrishna_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['prediction'] = np.where(df['score_5'] > 0.05, 1, 0)\n",
    "#df['prediction'] = [1 if x > 0.05 else 0 for x in df['score_5']]\n",
    "#df['prediction'] = list(np.where(df['score_5'] > 0.05, 1, 0)) \n",
    "# All three solutions raise the settingwithcopywarining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(dframe, ytrue = 'high_confidence_fraud_indicator', ypred = 'prediction', scores = 'score_5'):\n",
    "    tp = dframe[ytrue] * dframe[ypred]\n",
    "    fp = (1-dframe[ytrue]) * dframe[ypred]\n",
    "    fn =  dframe[ytrue] * (1-dframe[ypred])\n",
    "    \n",
    "    #calculating multiple metrics\n",
    "    precision = tp.sum()/(tp.sum() + fp.sum())\n",
    "    recall = tp.sum()/(tp.sum() + fn.sum())\n",
    "    f1score = f1_score(y_true = dframe[ytrue], y_pred = dframe[ypred], pos_label = 1)\n",
    "    auc_pr = average_precision_score(y_true = dframe[ytrue], y_score = dframe[scores], pos_label=1)\n",
    "    auc_roc = roc_auc_score(y_true = dframe[ytrue], y_score = dframe[scores])\n",
    "    \n",
    "    return pd.Series({'precision': precision, 'recall': recall, 'f1score': f1score, 'auc_pr':auc_pr, 'auc_roc':auc_roc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "byWeek_stats = df.groupby('entered_lp_week', as_index = False).apply(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The scope is always look like this so we did not need to change anything\n",
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "#Name of our Service Account Key\n",
    "google_key_file = 'service_key.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Worksheet 'test_sheet' id:0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the Worksheet ID\n",
    "spreadsheet_key = '14ROlpuOP9IkixM5-nn1Pc0ux6kWgmj7c62NzdDl-5hU'\n",
    "#This is the sheet name\n",
    "wks_name = 'raw_data'\n",
    "#We upload the tips data to our Google Sheet. Setting the row_names to False if you did not want the index to be included\n",
    "d2g.upload(byWeek_stats, spreadsheet_key, wks_name, credentials=credentials, row_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the Worksheet ID\n",
    "wb = gc.open_by_key('14ROlpuOP9IkixM5-nn1Pc0ux6kWgmj7c62NzdDl-5hU')\n",
    "#This is the sheet name\n",
    "wks_name = 'raw_data'\n",
    "ws = wb.worksheet(wks_name)\n",
    "\n",
    "#clear the existing data in worksheet\n",
    "ws.clear()\n",
    "\n",
    "#update new data to worksheet (first list out columns, and then add values for each column)\n",
    "ws.update([byWeek_stats.columns.values.tolist()] + byWeek_stats.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkrishna_env",
   "language": "python",
   "name": "gkrishna_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
